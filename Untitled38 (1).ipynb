{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe5f9b9-68c2-4841-8e9b-c44ba710d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef479a74-948a-4c42-8caa-1950f0bba627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341af9e7-c5d3-40bd-9154-fa0b308a7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#file_path = r\"C:\\Users\\Arnav\\Desktop\\horrid_henry.txt\"\n",
    "#if tokenizer.pad_token is None:\n",
    "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec2aad5-1506-4b08-b434-cc49ed3fff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_dataset(file_path, tokenizer):\n",
    "    # Load text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e723c9c-2ef7-419c-a12e-d757e67a49f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_and_preprocess_dataset(file_path, tokenizer):\\n    # Load text file\\n    with open(file_path, \\'r\\') as file:\\n        text = file.read().splitlines()\\n    \\n    # Create a dataset\\n    dataset = Dataset.from_dict({\"text\": text})\\n\\n    # Tokenize the dataset\\n    def tokenize_function(examples):\\n        return tokenizer(examples[\\'text\\'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\\n\\n    dataset = dataset.map(tokenize_function, batched=True)\\n    dataset = dataset.remove_columns([\\'text\\'])\\n    \\n    return dataset\\n '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_and_preprocess_dataset(file_path, tokenizer):\n",
    "    # Load text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read().splitlines()\n",
    "    \n",
    "    # Create a dataset\n",
    "    dataset = Dataset.from_dict({\"text\": text})\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset = dataset.remove_columns(['text'])\n",
    "    \n",
    "    return dataset\n",
    " '''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac9c018-6116-461a-b331-7fafa963b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Temp\\ipykernel_18852\\4118070727.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"tokenized_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "#The purpose of a data collator is for creation of attention masks and padding\n",
    "\n",
    "def create_data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "#dataset = load_and_preprocess_dataset(file_path, tokenizer)\n",
    "data_collator = create_data_collator(tokenizer)\n",
    "\n",
    "# Tokenize the dataset\n",
    "'''\n",
    "dataset = load_and_preprocess_dataset(file_path, tokenizer)\n",
    "\n",
    "# Save the tokenized dataset to a .pt file\n",
    "torch.save(dataset, \"tokenized_dataset.pt\")\n",
    "\n",
    "print(\"Tokenized dataset saved successfully.\")\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load the tokenized dataset from the .pt file\n",
    "dataset = torch.load(\"tokenized_dataset.pt\")\n",
    "\n",
    "print(\"Tokenized dataset loaded successfully.\")\n",
    "\n",
    "dir_location = r\"C:\\Users\\Arnav\\Desktop\\Gagan_tokenized\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151df674-7407-493b-a43a-03f8ea741774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved successfully at C:\\Users\\Arnav\\Desktop\\Gagan_tokenized\\tokenized_dataset.pt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnav\\AppData\\Local\\Temp\\ipykernel_18852\\2887989192.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"tokenized_dataset.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Specify the directory where you want to save the dataset\n",
    "dir_location = r\"C:\\Users\\Arnav\\Desktop\\Gagan_tokenized\"\n",
    "\n",
    "# Ensure the directory exists (optional, to avoid errors)\n",
    "if not os.path.exists(dir_location):\n",
    "    os.makedirs(dir_location)\n",
    "\n",
    "# Specify the full path to save the dataset\n",
    "save_path = os.path.join(dir_location, \"tokenized_dataset.pt\")\n",
    "\n",
    "# Load the tokenized dataset (if you haven't already done so)\n",
    "dataset = torch.load(\"tokenized_dataset.pt\")\n",
    "\n",
    "# Save the tokenized dataset to the specified directory\n",
    "torch.save(dataset, save_path)\n",
    "\n",
    "print(f\"Tokenized dataset saved successfully at {save_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ec3ec1-cbbc-4495-9f82-f5109d79c422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: torch in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from torch->transformers[torch]) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\anaconda3\\envs\\story_telling\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e6eb20-2846-4029-ac6a-d8891c76956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,  # Use more epochs for better fine-tuning\n",
    "    per_device_train_batch_size=10,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Fine-tune the model using Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5069e278-e9a0-41a8-b884-2b5ab6116b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model and tokenizer saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\n# Load the fine-tuned model and tokenizer from the directory\\nmodel = GPT2LMHeadModel.from_pretrained(\"./fine-tuned-gpt2\")\\ntokenizer = GPT2Tokenizer.from_pretrained(\"./fine-tuned-gpt2\")\\n\\nprint(\"Fine-tuned model and tokenizer loaded successfully.\")\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "\n",
    "trainer.save_model(\"./fine-tuned-gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt2\")\n",
    "\n",
    "print(\"Fine-tuned model and tokenizer saved successfully.\")\n",
    "\n",
    "'''\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from the directory\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine-tuned-gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine-tuned-gpt2\")\n",
    "\n",
    "print(\"Fine-tuned model and tokenizer loaded successfully.\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eceb244-e023-48e1-b86a-b614a41ed36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model and tokenizer saved successfully to C:\\Users\\Arnav\\Desktop\\gagan_finetuned_model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Specify the new directory\n",
    "new_dir_location = r\"C:\\Users\\Arnav\\Desktop\\gagan_finetuned_model\"\n",
    "\n",
    "# Save the fine-tuned model and tokenizer to the new directory\n",
    "trainer.save_model(new_dir_location)  # Save model weights and configuration\n",
    "tokenizer.save_pretrained(new_dir_location)  # Save tokenizer files\n",
    "\n",
    "print(f\"Fine-tuned model and tokenizer saved successfully to {new_dir_location}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d791b009-6dd7-4aff-badd-81ac2fb264a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully to C:\\Users\\Arnav\\Desktop\\gagan_tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Specify the new directory for the tokenizer\n",
    "tokenizer_dir_location = r\"C:\\Users\\Arnav\\Desktop\\gagan_tokenizer\"\n",
    "\n",
    "# Save the existing tokenizer to the new directory\n",
    "tokenizer.save_pretrained(tokenizer_dir_location)\n",
    "\n",
    "print(f\"Tokenizer saved successfully to {tokenizer_dir_location}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac4c276-2f74-4604-8099-7dddb6d9d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, a man in the presence of the Lord was not to be seen as a god but as an angel. And I say to you, He was a very wise man, and a wise angel, for He would not have ever been a son of a woman. He knew that He had a love for all men, but He said, \"This is not the love I desire\n",
      "\n",
      "\n",
      "Once upon a time, a blacksmith had the power to forge a stone with a single strike, and the ability to break it down into parts by hand, so he could use it as a weapon. But when he used his great strength to destroy the stone, it was broken down and made into a large rock.\n",
      "\n",
      "The Stone of the Blacksmith\n",
      ". The stone was a very hard\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same.\n",
      "\n",
      "\n",
      "Which one do you like the most 0, 1 or 2?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same.\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. When we are living together, we all have the ability and power to change\n",
      "\n",
      "\n",
      "Which one do you like the most 0, 1 or 2?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better life for us.\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no way out. As a result, we are divided and we have to struggle\n",
      "\n",
      "\n",
      "Which one do you like the most 0, 1 or 2?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. In this way, we will find our way.\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. That is what we are doing in the West today. This is the\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. And the reason that we are here is because we have the ability,\n",
      "\n",
      "\n",
      "Which one do you like the most 0, 1 or 2?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. That is what we are doing in the West today. This is the reason why the first generation of immigrants to America started the movement to bring a\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. That is what we are doing in the West today. This is the first step toward making our world a more peaceful place. The next step is\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. That is what we are doing in the West today. This is the new and important way of life. And that is why it is important that\n",
      "\n",
      "\n",
      "Which one do you like the most 0, 1 or 2?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final story is:\n",
      "\n",
      "\n",
      "Once upon a time, the people of the world knew that they were not alone.\n",
      "\n",
      "The world is divided into nations and cultures. There are many different kinds of people, and the way they talk and live is very different from their culture, their religion, or their country. These differences are very important to the development of human culture and our ability to survive. We are all the same. If we don't have a better way to live, there will be no better people to come to us and teach us how to make our lives better. That is what we are doing in the West today. This is the reason why the first generation of immigrants to America started the movement to bring a\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Starting text\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# Number of iterations for story generation\n",
    "num_iterations = 5\n",
    "\n",
    "# Base max_length\n",
    "base_max_length = 80\n",
    "\n",
    "# Generate the story\n",
    "for i in range(num_iterations):\n",
    "    # Adjust max_length for each iteration\n",
    "    current_max_length = base_max_length + (i * 15)\n",
    "    \n",
    "    # Generate text with added parameters\n",
    "    outputs = generator(\n",
    "        input_text, \n",
    "        max_length=current_max_length, \n",
    "        num_return_sequences=3, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.7,  # Lower temperature for more focused generations\n",
    "        top_k=50,  # Restrict to top 50 words\n",
    "        top_p=0.9,  # Nucleus sampling\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "        pad_token_id=tokenizer.pad_token_id  # Prevent abrupt padding\n",
    "    )\n",
    "    \n",
    "    # Print and select outputs\n",
    "    print(outputs[0]['generated_text'])\n",
    "    print(\"\\n\")\n",
    "    print(outputs[1]['generated_text'])\n",
    "    print(\"\\n\")\n",
    "    print(outputs[2]['generated_text'])\n",
    "    print(\"\\n\")\n",
    "    print(\"Which one do you like the most 0, 1 or 2?\")\n",
    "    print(\"\\n\")\n",
    "    choice = int(input())\n",
    "    input_text = outputs[choice]['generated_text']  # Update input_text with the selected generated text\n",
    "print(\"The final story is:\")\n",
    "print(\"\\n\")\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fb65d-bae1-42c7-b9e5-8418c9bf3094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
